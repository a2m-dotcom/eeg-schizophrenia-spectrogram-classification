{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:59:23.328139Z",
     "iopub.status.busy": "2025-11-19T09:59:23.327130Z",
     "iopub.status.idle": "2025-11-19T09:59:23.332006Z",
     "shell.execute_reply": "2025-11-19T09:59:23.331002Z",
     "shell.execute_reply.started": "2025-11-19T09:59:23.328113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==== Set your local data folder here ====\n",
    "DATA_DIR = \"data/EEG_IMAGES\"\n",
    "LABELS_CSV = \"data/labels.csv\"\n",
    "OUTPUT_DIR = \"outputs/\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1) Load labels and image metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T09:59:26.577950Z",
     "iopub.status.busy": "2025-11-19T09:59:26.577420Z",
     "iopub.status.idle": "2025-11-19T09:59:26.872033Z",
     "shell.execute_reply": "2025-11-19T09:59:26.870939Z",
     "shell.execute_reply.started": "2025-11-19T09:59:26.577923Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/eeg-spectrogram-images-for-schizophrenia-detection/labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_662/1120295682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABELS_CSV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'subject_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'session'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/eeg-spectrogram-images-for-schizophrenia-detection/labels.csv'"
     ]
    }
   ],
   "source": [
    "df_labels = pd.read_csv(LABELS_CSV)\n",
    "df_labels['subject_id'] = df_labels['subject_id'].str.replace('subject_', '').astype(int)\n",
    "df_labels['session'] = df_labels['session'].astype(int)\n",
    "df_labels['phase'] = df_labels['phase'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_filename(fname):\n",
    "    match = re.match(r\"subj_subject_(\\d+)_s(\\d+)_Phase_(\\d+)\\.png\", fname)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2)), int(match.group(3))\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "image_files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.png') or f.lower().endswith('.jpg')]\n",
    "df_images = pd.DataFrame({'filename': image_files})\n",
    "df_images[['subject_id', 'session', 'phase']] = df_images['filename'].apply(lambda x: pd.Series(parse_filename(x)))\n",
    "df_images.dropna(subset=['subject_id', 'session', 'phase'], inplace=True)\n",
    "df_images['subject_id'] = df_images['subject_id'].astype(int)\n",
    "df_images['session'] = df_images['session'].astype(int)\n",
    "df_images['phase'] = df_images['phase'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_images, df_labels, on=['subject_id','session','phase'], how='left')\n",
    "print(f\"Total images after merge: {len(df_merged)}\")\n",
    "print(f\"Missing labels: {df_merged['label'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if df_merged['label'].dtype != object:\n",
    "    df_merged['label'] = df_merged['label'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Plot class distribution bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='label', data=df_merged, order=df_merged['label'].value_counts().index)\n",
    "plt.title('Class distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR,'class_distribution.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Show at least 2 sample images per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# pick the correct filename column\n",
    "file_col = [c for c in df_merged.columns if \"filename\" in c][0]\n",
    "\n",
    "classes = df_merged['label'].unique()\n",
    "plt.figure(figsize=(8, 3*len(classes)))\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    samples = df_merged[df_merged['label']==cls][file_col].values[:2]\n",
    "    for j, fname in enumerate(samples):\n",
    "        plt.subplot(len(classes), 2, i*2 + j + 1)\n",
    "        img = load_img(os.path.join(DATA_DIR, fname), target_size=(224,224))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Class: {cls}\", fontsize=10)\n",
    "\n",
    "plt.suptitle(\"2 Sample Images per Class\", fontsize=14)\n",
    "plt.tight_layout(rect=[0,0,1,0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainval_df, test_df = train_test_split(df_merged, test_size=0.20, stratify=df_merged['label'], random_state=40)\n",
    "train_df, val_df = train_test_split(trainval_df, test_size=0.20, stratify=trainval_df['label'], random_state=40)\n",
    "\n",
    "print('Train:', len(train_df), 'Val:', len(val_df), 'Test:', len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) ImageDataGenerators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = (224,224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1\n",
    ")\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Use the correct filename column\n",
    "file_col = [c for c in train_df.columns if \"filename\" in c][0]\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    directory=DATA_DIR,\n",
    "    x_col=file_col,\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    directory=DATA_DIR,\n",
    "    x_col=file_col,\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Important: set shuffle=False for test generator to align predictions\n",
    "test_gen = test_datagen.flow_from_dataframe(\n",
    "    test_df,\n",
    "    directory=DATA_DIR,\n",
    "    x_col=file_col,\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "NUM_CLASSES = len(train_gen.class_indices)\n",
    "CLASS_INDICES = {v: k for k, v in train_gen.class_indices.items()}\n",
    "print('Classes:', train_gen.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16, ResNet50, MobileNetV2\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LR = 1e-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/kaggle/input/eeg-spectrogram-images-for-schizophrenia-detection/EEG_IMAGES_VGG16-20250724T160523Z-1-001/EEG_IMAGES_VGG16\"\n",
    "LABELS_CSV = \"/kaggle/input/eeg-spectrogram-images-for-schizophrenia-detection/labels.csv\"\n",
    "\n",
    "df_labels = pd.read_csv(LABELS_CSV)\n",
    "\n",
    "# Clean subject_id column\n",
    "df_labels['subject_id'] = df_labels['subject_id'].str.replace('subject_', '').astype(int)\n",
    "df_labels['session'] = df_labels['session'].astype(int)\n",
    "df_labels['phase'] = df_labels['phase'].astype(int)\n",
    "\n",
    "# 2. Parse subject_id, session, phase from image filenames\n",
    "def parse_filename(fname):\n",
    "    # example filename: subj_subject_135_s1_Phase_4.png\n",
    "    match = re.match(r\"subj_subject_(\\d+)_s(\\d+)_Phase_(\\d+)\\.png\", fname)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2)), int(match.group(3))\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "image_files = os.listdir(DATA_DIR)\n",
    "df_images = pd.DataFrame({'filename': image_files})\n",
    "\n",
    "df_images[['subject_id', 'session', 'phase']] = df_images['filename'].apply(\n",
    "    lambda x: pd.Series(parse_filename(x))\n",
    ")\n",
    "\n",
    "# Drop images with failed parsing\n",
    "df_images.dropna(subset=['subject_id', 'session', 'phase'], inplace=True)\n",
    "df_images['subject_id'] = df_images['subject_id'].astype(int)\n",
    "df_images['session'] = df_images['session'].astype(int)\n",
    "df_images['phase'] = df_images['phase'].astype(int)\n",
    "\n",
    "# 3. Merge image info with labels\n",
    "df_merged = pd.merge(\n",
    "    df_images,\n",
    "    df_labels,\n",
    "    on=['subject_id', 'session', 'phase'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Total images after merge: {len(df_merged)}\")\n",
    "print(f\"Missing labels: {df_merged['label'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2. Parse subject_id, session, phase from image filenames\n",
    "def parse_filename(fname):\n",
    "    # example filename: subj_subject_135_s1_Phase_4.png\n",
    "    match = re.match(r\"subj_subject_(\\d+)_s(\\d+)_Phase_(\\d+)\\.png\", fname)\n",
    "    if match:\n",
    "        return int(match.group(1)), int(match.group(2)), int(match.group(3))\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "image_files = os.listdir(DATA_DIR)\n",
    "df_images = pd.DataFrame({'filename': image_files})\n",
    "\n",
    "df_images[['subject_id', 'session', 'phase']] = df_images['filename'].apply(\n",
    "    lambda x: pd.Series(parse_filename(x))\n",
    ")\n",
    "\n",
    "# Drop images with failed parsing\n",
    "df_images.dropna(subset=['subject_id', 'session', 'phase'], inplace=True)\n",
    "df_images['subject_id'] = df_images['subject_id'].astype(int)\n",
    "df_images['session'] = df_images['session'].astype(int)\n",
    "df_images['phase'] = df_images['phase'].astype(int)\n",
    "\n",
    "# 3. Merge image info with labels\n",
    "df_merged = pd.merge(\n",
    "    df_images,\n",
    "    df_labels,\n",
    "    on=['subject_id', 'session', 'phase'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"Total images after merge: {len(df_merged)}\")\n",
    "print(f\"Missing labels: {df_merged['label'].isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing labels\n",
    "df_merged.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "# Show label distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(y=df_merged['label'], order=df_merged['label'].value_counts().index)\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(df_merged['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4. Show 1 sample image per label\n",
    "labels = df_merged['label'].unique()\n",
    "plt.figure(figsize=(16,4))\n",
    "for i, label in enumerate(labels):\n",
    "    sample_img = df_merged[df_merged['label'] == label]['filename_x'].values[0]\n",
    "    img = plt.imread(os.path.join(DATA_DIR, sample_img))\n",
    "    plt.subplot(1, len(labels), i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 5. Train/val/test split (stratified)\n",
    "trainval_df, test_df = train_test_split(df_merged, test_size=0.15, stratify=df_merged['label'], random_state=42)\n",
    "train_df, val_df = train_test_split(trainval_df, test_size=0.1765, stratify=trainval_df['label'], random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# 6. Data generators\n",
    "IMG_SIZE = (224,224)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=DATA_DIR,\n",
    "    x_col='filename_x',\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_gen = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory=DATA_DIR,\n",
    "    x_col='filename_x',\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_gen = val_test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=DATA_DIR,\n",
    "    x_col='filename_x',\n",
    "    y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Model Definition ---\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(2,2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_gen.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# --- Define Confidence Threshold ---\n",
    "CONFIDENCE_THRESHOLD = 0.8 \n",
    "\n",
    "class HighConfidenceAccuracy(Callback):\n",
    "    def __init__(self, train_gen, val_gen, threshold=0.8):\n",
    "        super(HighConfidenceAccuracy, self).__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.threshold = threshold\n",
    "        self.hca_train = []   \n",
    "        self.hca_val = []     \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        def get_high_conf_acc(generator):\n",
    "            y_true_all = np.array(generator.classes) \n",
    "            y_pred_prob = self.model.predict(generator, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            max_probs = np.max(y_pred_prob, axis=1)\n",
    "            mask = max_probs >= self.threshold\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            y_true_filtered = y_true_all[mask]  \n",
    "            y_pred_filtered = y_pred[mask]\n",
    "\n",
    "            acc = np.mean(y_true_filtered == y_pred_filtered)\n",
    "            return float(acc)\n",
    "\n",
    "        train_hca = get_high_conf_acc(self.train_gen)\n",
    "        val_hca = get_high_conf_acc(self.val_gen)\n",
    "\n",
    "        self.hca_train.append(train_hca)\n",
    "        self.hca_val.append(val_hca)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} - \"\n",
    "              f\"Train Acc: {train_hca:.4f} \"\n",
    "              f\"Val Acc: {val_hca:.4f}\")\n",
    "\n",
    "# --- Train with Fixed Callback ---\n",
    "callback = HighConfidenceAccuracy(train_gen, val_gen, threshold=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=40,\n",
    "    callbacks=[callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_true_test = np.array(test_gen.classes) \n",
    "y_pred_prob_test = model.predict(test_gen, verbose=0)\n",
    "y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n",
    "max_probs_test = np.max(y_pred_prob_test, axis=1)\n",
    "\n",
    "mask_test = max_probs_test >= CONFIDENCE_THRESHOLD\n",
    "y_true_high = y_true_test[mask_test]\n",
    "y_pred_high = y_pred_test[mask_test]\n",
    "\n",
    "if len(y_true_high) > 0:\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_high, y_pred_high)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix (Test - High Confidence Only)')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true_high, y_pred_high, target_names=class_names))\n",
    "else:\n",
    "    print(\"No high-confidence predictions in test set. Lower the threshold.\")\n",
    "\n",
    "# --- Plot Training History ---\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Standard Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Standard Accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save(\"CustomCNN_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Create a link to download\n",
    "FileLink(\"CustomCNN_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  VGG16 Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "IMG_SIZE = (224, 224)  \n",
    "CONFIDENCE_THRESHOLD = 0.8\n",
    "\n",
    "# --- Load Pretrained VGG16 Base ---\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "base_model.trainable = False  \n",
    "\n",
    "# --- Build Model on Top ---\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(), \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_gen.class_indices), activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# --- Custom Callback: High Confidence Accuracy ---\n",
    "class HighConfidenceAccuracy(Callback):\n",
    "    def __init__(self, train_gen, val_gen, threshold=0.8):\n",
    "        super(HighConfidenceAccuracy, self).__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.threshold = threshold\n",
    "        self.hca_train = []   \n",
    "        self.hca_val = []     \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        def get_high_conf_acc(generator):\n",
    "            y_true_all = np.array(generator.classes) \n",
    "            y_pred_prob = self.model.predict(generator, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            max_probs = np.max(y_pred_prob, axis=1)\n",
    "            mask = max_probs >= self.threshold\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            y_true_filtered = y_true_all[mask]  \n",
    "            y_pred_filtered = y_pred[mask]\n",
    "\n",
    "            acc = np.mean(y_true_filtered == y_pred_filtered)\n",
    "            return float(acc)\n",
    "\n",
    "        train_hca = get_high_conf_acc(self.train_gen)\n",
    "        val_hca = get_high_conf_acc(self.val_gen)\n",
    "\n",
    "        self.hca_train.append(train_hca)\n",
    "        self.hca_val.append(val_hca)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} - \"\n",
    "              f\"Train HCA: {train_hca:.4f} \"\n",
    "              f\"Val HCA: {val_hca:.4f}\")\n",
    "\n",
    "# --- Train Model ---\n",
    "callback = HighConfidenceAccuracy(train_gen, val_gen, threshold=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=40,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_true_test = np.array(test_gen.classes)\n",
    "y_pred_prob_test = model.predict(test_gen, verbose=0)\n",
    "y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n",
    "max_probs_test = np.max(y_pred_prob_test, axis=1)\n",
    "\n",
    "mask_test = max_probs_test >= CONFIDENCE_THRESHOLD\n",
    "y_true_high = y_true_test[mask_test]\n",
    "y_pred_high = y_pred_test[mask_test]\n",
    "\n",
    "if len(y_true_high) > 0:\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_high, y_pred_high)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix (Test ‚â• {CONFIDENCE_THRESHOLD})')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(y_true_high, y_pred_high, target_names=class_names))\n",
    "else:\n",
    "    print(f\"No predictions with confidence ‚â• {CONFIDENCE_THRESHOLD}. Consider lowering threshold.\")\n",
    "\n",
    "# --- Plot Training History ---\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Standard Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "plt.title('Standard Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, callback.hca_train, label='Train HCA', marker='o')\n",
    "plt.plot(epochs, callback.hca_val, label='Val HCA', marker='s')\n",
    "plt.title(f'Accuracy (Threshold={CONFIDENCE_THRESHOLD})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('HCA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2  # ‚Üê CHANGED\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "CONFIDENCE_THRESHOLD = 0.6  \n",
    "\n",
    "# --- Load Pretrained MobileNetV2 Base ---\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "base_model.trainable = False \n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_gen.class_indices), activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "class HighConfidenceAccuracy(Callback):\n",
    "    def __init__(self, train_gen, val_gen, threshold=0.6):\n",
    "        super(HighConfidenceAccuracy, self).__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.threshold = threshold\n",
    "        self.hca_train = []\n",
    "        self.hca_val = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        def get_high_conf_acc(generator):\n",
    "            y_true_all = np.array(generator.classes)\n",
    "            y_pred_prob = self.model.predict(generator, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            max_probs = np.max(y_pred_prob, axis=1)\n",
    "            mask = max_probs >= self.threshold\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            y_true_filtered = y_true_all[mask]\n",
    "            y_pred_filtered = y_pred[mask]\n",
    "\n",
    "            acc = np.mean(y_true_filtered == y_pred_filtered)\n",
    "            return float(acc)\n",
    "\n",
    "        train_hca = get_high_conf_acc(self.train_gen)\n",
    "        val_hca = get_high_conf_acc(self.val_gen)\n",
    "\n",
    "        self.hca_train.append(train_hca)\n",
    "        self.hca_val.append(val_hca)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} - \"\n",
    "              f\"Train HCA: {train_hca:.4f} \"\n",
    "              f\"Val HCA: {val_hca:.4f}\")\n",
    "\n",
    "# --- Train Model ---\n",
    "callback = HighConfidenceAccuracy(train_gen, val_gen, threshold=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Evaluate on Test Set \n",
    "y_true_test = np.array(test_gen.classes)\n",
    "y_pred_prob_test = model.predict(test_gen, verbose=0)\n",
    "y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n",
    "max_probs_test = np.max(y_pred_prob_test, axis=1)\n",
    "\n",
    "# --- Diagnostic\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(max_probs_test, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold = {CONFIDENCE_THRESHOLD}')\n",
    "plt.title('Distribution of Maximum Prediction Probabilities (Test Set)')\n",
    "plt.xlabel('Confidence (Max Softmax Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Confidence Stats:\")\n",
    "print(f\"Max confidence: {max_probs_test.max():.4f}\")\n",
    "print(f\"Min confidence: {max_probs_test.min():.4f}\")\n",
    "print(f\"Mean confidence: {max_probs_test.mean():.4f}\")\n",
    "print(f\"Threshold: {CONFIDENCE_THRESHOLD}\\n\")\n",
    "\n",
    "# --- Apply Threshold ---\n",
    "mask_test = max_probs_test >= CONFIDENCE_THRESHOLD\n",
    "y_true_high = y_true_test[mask_test]\n",
    "y_pred_high = y_pred_test[mask_test]\n",
    "\n",
    "if len(y_true_high) > 0:\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_high, y_pred_high)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix (Test ‚â• {CONFIDENCE_THRESHOLD})')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== Classification Report ===\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_true_high, y_pred_high, target_names=class_names))\n",
    "else:\n",
    "    print(f\"\\n‚ùå No predictions with confidence ‚â• {CONFIDENCE_THRESHOLD}. Consider lowering threshold further (e.g., 0.5).\")\n",
    "\n",
    "# --- Plot Training History ---\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Standard Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "plt.title('Standard Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# High Confidence Accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, callback.hca_train, label='Train HCA', marker='o')\n",
    "plt.plot(epochs, callback.hca_val, label='Val HCA', marker='s')\n",
    "plt.title(f'High Confidence Accuracy (Threshold={CONFIDENCE_THRESHOLD})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('HCA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import DenseNet121  \n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)  \n",
    "CONFIDENCE_THRESHOLD = 0.5  \n",
    "\n",
    "# --- Load Pretrained DenseNet121 Base ---\n",
    "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "base_model.trainable = False \n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_gen.class_indices), activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "class HighConfidenceAccuracy(Callback):\n",
    "    def __init__(self, train_gen, val_gen, threshold=0.5):  # ‚Üê Updated default\n",
    "        super(HighConfidenceAccuracy, self).__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.threshold = threshold\n",
    "        self.hca_train = []\n",
    "        self.hca_val = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        def get_high_conf_acc(generator):\n",
    "            y_true_all = np.array(generator.classes)\n",
    "            y_pred_prob = self.model.predict(generator, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            max_probs = np.max(y_pred_prob, axis=1)\n",
    "            mask = max_probs >= self.threshold\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            y_true_filtered = y_true_all[mask]\n",
    "            y_pred_filtered = y_pred[mask]\n",
    "\n",
    "            acc = np.mean(y_true_filtered == y_pred_filtered)\n",
    "            return float(acc)\n",
    "\n",
    "        train_hca = get_high_conf_acc(self.train_gen)\n",
    "        val_hca = get_high_conf_acc(self.val_gen)\n",
    "\n",
    "        self.hca_train.append(train_hca)\n",
    "        self.hca_val.append(val_hca)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} - \"\n",
    "              f\"Train HCA: {train_hca:.4f} \"\n",
    "              f\"Val HCA: {val_hca:.4f}\")\n",
    "\n",
    "\n",
    "# --- Train Model ---\n",
    "callback = HighConfidenceAccuracy(train_gen, val_gen, threshold=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=20,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_true_test = np.array(test_gen.classes)\n",
    "y_pred_prob_test = model.predict(test_gen, verbose=0)\n",
    "y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n",
    "max_probs_test = np.max(y_pred_prob_test, axis=1)\n",
    "\n",
    "\n",
    "# --- Diagnostic: Confidence Distribution ---\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(max_probs_test, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold = {CONFIDENCE_THRESHOLD}')\n",
    "plt.title('Distribution of Maximum Prediction Probabilities (Test Set)')\n",
    "plt.xlabel('Confidence (Max Softmax Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Confidence Stats:\")\n",
    "print(f\"Max confidence: {max_probs_test.max():.4f}\")\n",
    "print(f\"Min confidence: {max_probs_test.min():.4f}\")\n",
    "print(f\"Mean confidence: {max_probs_test.mean():.4f}\")\n",
    "print(f\"Threshold: {CONFIDENCE_THRESHOLD}\\n\")\n",
    "\n",
    "\n",
    "# --- Apply Threshold ---\n",
    "mask_test = max_probs_test >= CONFIDENCE_THRESHOLD\n",
    "y_true_high = y_true_test[mask_test]\n",
    "y_pred_high = y_pred_test[mask_test]\n",
    "\n",
    "if len(y_true_high) > 0:\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_high, y_pred_high)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix (Test ‚â• {CONFIDENCE_THRESHOLD})')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== Classification Report ===\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_true_high, y_pred_high, target_names=class_names))\n",
    "else:\n",
    "    print(f\"\\n‚ùå Still no predictions with confidence ‚â• {CONFIDENCE_THRESHOLD}. Consider lowering further (e.g., 0.4).\")\n",
    "\n",
    "\n",
    "# --- Plot Training History ---\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Standard Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "plt.title('Standard Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# High Confidence Accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, callback.hca_train, label='Train HCA', marker='o')\n",
    "plt.plot(epochs, callback.hca_val, label='Val HCA', marker='s')\n",
    "plt.title(f'High Confidence Accuracy (Threshold={CONFIDENCE_THRESHOLD})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('HCA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50  \n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)  \n",
    "CONFIDENCE_THRESHOLD = 0.5  \n",
    "\n",
    "# --- Load Pretrained ResNet50 Base ---\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "base_model.trainable = False \n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_gen.class_indices), activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "class HighConfidenceAccuracy(Callback):\n",
    "    def __init__(self, train_gen, val_gen, threshold=0.5):  \n",
    "        super(HighConfidenceAccuracy, self).__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.threshold = threshold\n",
    "        self.hca_train = []\n",
    "        self.hca_val = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        def get_high_conf_acc(generator):\n",
    "            y_true_all = np.array(generator.classes)\n",
    "            y_pred_prob = self.model.predict(generator, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            max_probs = np.max(y_pred_prob, axis=1)\n",
    "            mask = max_probs >= self.threshold\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            y_true_filtered = y_true_all[mask]\n",
    "            y_pred_filtered = y_pred[mask]\n",
    "\n",
    "            acc = np.mean(y_true_filtered == y_pred_filtered)\n",
    "            return float(acc)\n",
    "\n",
    "        train_hca = get_high_conf_acc(self.train_gen)\n",
    "        val_hca = get_high_conf_acc(self.val_gen)\n",
    "\n",
    "        self.hca_train.append(train_hca)\n",
    "        self.hca_val.append(val_hca)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} - \"\n",
    "              f\"Train HCA: {train_hca:.4f} \"\n",
    "              f\"Val HCA: {val_hca:.4f}\")\n",
    "\n",
    "\n",
    "# --- Train Model ---\n",
    "callback = HighConfidenceAccuracy(train_gen, val_gen, threshold=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=40,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_true_test = np.array(test_gen.classes)\n",
    "y_pred_prob_test = model.predict(test_gen, verbose=0)\n",
    "y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n",
    "max_probs_test = np.max(y_pred_prob_test, axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(max_probs_test, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold = {CONFIDENCE_THRESHOLD}')\n",
    "plt.title('Distribution of Maximum Prediction Probabilities (Test Set)')\n",
    "plt.xlabel('Confidence (Max Softmax Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Confidence Stats:\")\n",
    "print(f\"Max confidence: {max_probs_test.max():.4f}\")\n",
    "print(f\"Min confidence: {max_probs_test.min():.4f}\")\n",
    "print(f\"Mean confidence: {max_probs_test.mean():.4f}\")\n",
    "print(f\"Threshold: {CONFIDENCE_THRESHOLD}\\n\")\n",
    "\n",
    "\n",
    "# --- Apply Threshold ---\n",
    "mask_test = max_probs_test >= CONFIDENCE_THRESHOLD\n",
    "y_true_high = y_true_test[mask_test]\n",
    "y_pred_high = y_pred_test[mask_test]\n",
    "\n",
    "if len(y_true_high) > 0:\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_high, y_pred_high)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix (Test ‚â• {CONFIDENCE_THRESHOLD})')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== Classification Report ===\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_true_high, y_pred_high, target_names=class_names))\n",
    "else:\n",
    "    print(f\"\\n‚ùå Still no predictions with confidence ‚â• {CONFIDENCE_THRESHOLD}. Consider lowering further (e.g., 0.4).\")\n",
    "\n",
    "\n",
    "# --- Plot Training History ---\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Standard Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "plt.title('Standard Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# High Confidence Accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, callback.hca_train, label='Train HCA', marker='o')\n",
    "plt.plot(epochs, callback.hca_val, label='Val HCA', marker='s')\n",
    "plt.title(f'Accuracy (Threshold={CONFIDENCE_THRESHOLD})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('HCA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0  \n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "IMG_SIZE = (224, 224)  \n",
    "CONFIDENCE_THRESHOLD = 0.5  \n",
    "\n",
    "\n",
    "# --- Load EfficientNetB0 Base ---\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "base_model.trainable = False  # Freeze base\n",
    "\n",
    "# --- Build Model ---\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(train_gen.class_indices), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "class HighConfidenceAccuracy(Callback):\n",
    "    def __init__(self, train_gen, val_gen, threshold=0.5):  \n",
    "        super(HighConfidenceAccuracy, self).__init__()\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.threshold = threshold\n",
    "        self.hca_train = []\n",
    "        self.hca_val = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        def get_high_conf_acc(generator):\n",
    "            y_true_all = np.array(generator.classes)\n",
    "            y_pred_prob = self.model.predict(generator, verbose=0)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "            max_probs = np.max(y_pred_prob, axis=1)\n",
    "            mask = max_probs >= self.threshold\n",
    "\n",
    "            if np.sum(mask) == 0:\n",
    "                return 0.0\n",
    "\n",
    "            y_true_filtered = y_true_all[mask]\n",
    "            y_pred_filtered = y_pred[mask]\n",
    "\n",
    "            acc = np.mean(y_true_filtered == y_pred_filtered)\n",
    "            return float(acc)\n",
    "\n",
    "        train_hca = get_high_conf_acc(self.train_gen)\n",
    "        val_hca = get_high_conf_acc(self.val_gen)\n",
    "\n",
    "        self.hca_train.append(train_hca)\n",
    "        self.hca_val.append(val_hca)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch + 1} - \"\n",
    "              f\"Train HCA: {train_hca:.4f} \"\n",
    "              f\"Val HCA: {val_hca:.4f}\")\n",
    "\n",
    "\n",
    "# --- Train Model ---\n",
    "callback = HighConfidenceAccuracy(train_gen, val_gen, threshold=CONFIDENCE_THRESHOLD)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=40,\n",
    "    callbacks=[callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "y_true_test = np.array(test_gen.classes)\n",
    "y_pred_prob_test = model.predict(test_gen, verbose=0)\n",
    "y_pred_test = np.argmax(y_pred_prob_test, axis=1)\n",
    "max_probs_test = np.max(y_pred_prob_test, axis=1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.hist(max_probs_test, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold = {CONFIDENCE_THRESHOLD}')\n",
    "plt.title('Distribution of Maximum Prediction Probabilities (Test Set)')\n",
    "plt.xlabel('Confidence (Max Softmax Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Confidence Stats:\")\n",
    "print(f\"Max confidence: {max_probs_test.max():.4f}\")\n",
    "print(f\"Min confidence: {max_probs_test.min():.4f}\")\n",
    "print(f\"Mean confidence: {max_probs_test.mean():.4f}\")\n",
    "print(f\"Threshold: {CONFIDENCE_THRESHOLD}\\n\")\n",
    "\n",
    "\n",
    "# --- Apply Threshold ---\n",
    "mask_test = max_probs_test >= CONFIDENCE_THRESHOLD\n",
    "y_true_high = y_true_test[mask_test]\n",
    "y_pred_high = y_pred_test[mask_test]\n",
    "\n",
    "if len(y_true_high) > 0:\n",
    "    class_names = list(test_gen.class_indices.keys())\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true_high, y_pred_high)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix (Test ‚â• {CONFIDENCE_THRESHOLD})')\n",
    "    plt.show()\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== Classification Report ===\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_true_high, y_pred_high, target_names=class_names))\n",
    "else:\n",
    "    print(f\"\\n‚ùå Still no predictions with confidence ‚â• {CONFIDENCE_THRESHOLD}. Consider lowering further (e.g., 0.4).\")\n",
    "\n",
    "\n",
    "# --- Plot Training History ---\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Standard Accuracy\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "plt.title('Standard Accuracy per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, history.history['loss'], label='Train Loss', marker='o')\n",
    "plt.plot(epochs, history.history['val_loss'], label='Val Loss', marker='s')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# High Confidence Accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, callback.hca_train, label='Train HCA', marker='o')\n",
    "plt.plot(epochs, callback.hca_val, label='Val HCA', marker='s')\n",
    "plt.title(f'Accuracy (Threshold={CONFIDENCE_THRESHOLD})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('HCA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your accuracy data\n",
    "models = ['CNN', 'VGG16', 'MobileNet', 'DenseNet', 'ResNet50']\n",
    "accuracy = [0.90, 0.97, 0.82, 0.79, 0.52]\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracy, color=['#4CAF50', '#2196F3', '#FF9800', '#9C27B0', '#F44336'],\n",
    "               edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# Annotate bars with accuracy values\n",
    "for bar, acc in zip(bars, accuracy):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "             f'{acc:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "\n",
    "# Customize\n",
    "plt.title('üìä Model Accuracy Comparison', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.ylim(0, 1.05)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7936247,
     "sourceId": 12567227,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
